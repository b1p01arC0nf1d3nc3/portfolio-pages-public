[{"content":"Bio #\rCurrent cybersecurity student with a background in teaching looking to explore and learn more about threat hunting, red teaming, and incident response.\nFirst fell in love with technology as a youngster by being enthralled by the speed of a blue hedgehog darting about a screen (an excellent distraction device employed by family). Since then, have taken a roundabout journey into tech and currently working towards getting certified in the area of penetration testing.\nThe aspect of giving back to the community in this field really appeals to me, and I look forward to sharing discoveries on my way with those already in the field and those like me who are finding their path into it.\n","date":"9 December 2023","permalink":"/portfolio-pages-public/posts/about-me/","section":"Posts","summary":"Bio #\rCurrent cybersecurity student with a background in teaching looking to explore and learn more about threat hunting, red teaming, and incident response.","title":"About Me"},{"content":"","date":"9 December 2023","permalink":"/portfolio-pages-public/","section":"b1p01arc0nf1d3nc3 - Portfolio","summary":"","title":"b1p01arc0nf1d3nc3 - Portfolio"},{"content":"","date":"9 December 2023","permalink":"/portfolio-pages-public/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"Review and experience of how the pathway added to my knowledge and what I want to focus on next.\n","date":"9 December 2023","permalink":"/portfolio-pages-public/posts/thm-red-team-pathway/","section":"Posts","summary":"Review and experience of how the pathway added to my knowledge and what I want to focus on next.","title":"THM Red Team Pathway"},{"content":"Systems vs. Security #\rDuring a cross-program project between systems administration and cyber security students, the cyber security students were tasked with breaking into a stand-alone server administered by the systems students. We broke off into teams and were given the IP address of our target server.\nOur team chose the name \u0026lsquo;Ping Pirates\u0026rsquo; and began enumerating the target machine. Using common tools such as nmap to find out which ports were open and what services were running, we discovered a limited range of ports and a dashboard running on a random ephemeral port.\nAmong the open ports was port 21, potentially offering ftp access. We enumerated this port more closely and found that it offered anonymous login, allowing us to access a staff list which could then be used to generate a list of usernames. Once we generated a list of potential usernames following common patterns (first.last, last.first, firstinitial.last, etc.), we did some quick brute force attacks to see if there were any accounts with weak passwords. After running these brute attacks with larger wordlists, it became apparent that this would not be a viable means of gaining initial access; this was made painfully obvious when we later discovered that all passwords were above 14 characters in length, with the most privileged passwords randomly generated and up to 20 characters.\nWe then changed focus and tried to assess how the systems students would be using the machine and looked into intercepting traffic to gain more intel. We knew they had the standard RDP port open and that they had a dashboard for a monitoring platform. We decided the best thing to do would be to try and relay traffic to either (a) intercept a login to their monitoring platform, as it did not use https, or (b) use a tool like Responder to relay traffic and potentially gain hashes.\n","date":"9 December 2023","permalink":"/portfolio-pages-public/posts/red-vs.-blue/","section":"Posts","summary":"Systems vs.","title":"Red vs. Blue"},{"content":"This will be a walkthrough of the malware dev project.\n","date":"9 December 2023","permalink":"/portfolio-pages-public/posts/dll-sideloading/","section":"Posts","summary":"This will be a walkthrough of the malware dev project.","title":"Dll Sideloading"},{"content":"How to play and enjoy Backdoors and Breaches #\rBackdoors and Breaches is a fantastic incident response activity that challenges participants to solve a multi-faceted cyber incident by making decisions and watching everything go wrong (well, at least some of the time).\nThe activity if free to try and is made by the fabulous folks over at Black Hills Information Security, who can be found here. Amazingly, the activity is even hosted online for all to use, and the web version makes it easy to host a remote session without requiring extensive preparation (you can find out more about the different ways to play here).\nIn terms of how the activity works, the incident responders (the participants) have 10 rounds to try and deal with the incident before it becomes too impactful. They need to discuss what they would like to and prioritize remediatory actions to attempt a full resolution. They can use procedure cards, such as SIEM log analysis and network threat hunting, to try and uncover one of the four elements of the incident. If the team successfully manage to uncover all four elements of the incident in time then they have handled the incident. There are a few other rules which can significantly affect play, and the main rules are summarized below.\nRules #\rWork as a team to discover all 4 cards to respond to the incident. 10 rounds to discover all 4 event cards. Actions can be chosen from the procedure cards. Actions must be reached via consensus (can also ask for specific clarification from the incident leader). A dice roll is made to determine if an action is successful or not. 11 and over is a success, while 10 and below is a failure. There is a 3-round cooldown after using a card to prevent the same card being used repeatedly. An \u0026lsquo;inject\u0026rsquo; card is added on a roll of 1 or 20, or after 3 consecutive failures. Inject cards add a random element to the game and can be positive or very destructive. These cards often lead to the most significant learning as participants are forced to react quickly to the change. Though not necessarily a rule, the following are recommendations to get more out of the experience:\nAfter a failure, ask for why that might have happened. Participants will often be able to give a justification for you, especially if they know of a weakness in their environment. The failure could, for example, be due to resource constraints, human errors, or policy changes. After the game, go through and assess if what happened was realistic. Participants will generally respond with a Yes, No, or Maybe. Always clarify Maybe. The time spent clarifying is well worth it due to the extra insight participants will get into their environment and you will also learn how to make the activity more representative next time. Allow for unique elements and changes to rules. The rules are a solid framework, but as the incident leader you have the ability to incorporate the suggestions and ideas of the participants to make the activity more successful. This could include revealing a slight hint due to the specificity of what the participants attempt to do, adding the ability to call a consultant if it fits the overall narrative, or guaranteeing a success for a procedure that the participants show excellence in. The way the activity is run by the incident leader obviously has a large impact on the success of the activity, but it is also important to get participants to buy in to the activity in the early stages by adding humor or appealing to what they are looking for. As a member of a cybersecurity club, this has often revolved around incorporation of our mascot: a hacking goose. This goose has been added to a range of different companies, and I always try my best to try and make a logo that includes the mascot and a slightly altered company name. This hopefully not only prompts a smile, but also sets the context for where the incident and the activity take place.\nAdding flavor with customized companies helps players become more involved in the scenario while also providing scope for the activity.\rSometimes, the sillier the company sounds, the more invested participants become.\rFrom the chances I\u0026rsquo;ve had to lead this activity, I can definitely vouch for its applicability to various groups in the cyber space. It has been a great tool for students as it allows them to apply the knowledge they are gaining throughout whatever course they may be doing to an activity where they are forced to justify decisions. Talking through options help reinforce learning and builds a degree of confidence when sharing this kind of information with those less focused on cyber. I can also see the value it would hold for incident response teams at different organizations, who would benefit from the opportunity to review incident response plans and identify areas that might be a bit foggy, thus prompting them to clarify procedures and communication trees to enable them to effectively respond to incidents.\nIf you can, I wholeheartedly recommend trying the activity out. If you want some tips, check out the video provided by BHIS to find out more.\n","date":"9 December 2023","permalink":"/portfolio-pages-public/posts/backdoors-and-breaches/","section":"Posts","summary":"How to play and enjoy Backdoors and Breaches #\rBackdoors and Breaches is a fantastic incident response activity that challenges participants to solve a multi-faceted cyber incident by making decisions and watching everything go wrong (well, at least some of the time).","title":"Backdoors and Breaches"},{"content":"During a work term, I was given the chance to build out some open source tools that can be used to create a streamlined security operations center. It was a very valuable experience and helped me develop a better understanding of tooling available to organizations that may have resource limitations.\nHere are some of the configuration steps and code that was used in the setup of the Hive server, a self-contained testing server to explore open-source SOC options.\nBase OS: CentOS 7. Requirements: Depending on the number of products you want to test at once, this could range from 8 - 32 GB.\nDocker engine: simple install using yum: sudo yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\nOnce docker is set up, start the service and check the inital hello world container can run. If it does, then we can start building the template for the SOC from the hosted page on github.\nPortainer #\rUsing Portainer will make managing the large number of containers and images simpler. The container for Portainer can be installed by following the steps in this link:\nInstall Portainer with Docker on Linux - Portainer Documentation\nThe basic steps are given below.\ndocker volume name_of_data_storage_for_portainer\nThe above command will create a docker volume for Portainer. We can then run the docker command to pull in the image and map the ports we will use to connect to the web interface for managing containers.\nI used:\ndocker run -d -p 8000:8000 -p 9443:9443 --name portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v name_of_data_storage_for_portainer:/data portainer/portainer-ce:2.9.3\nthough your command may look different depending on the versions and the ports you select.\nFor the web interface, Portainer will automatically create a self-signed certificate, but this can be changed both during and after installation, should this be a production environment.\nSOC Components #\rThe following tools were used to create the SOC, providing a wide range of functionality for small teams of security analysts.\nTheHive - incident platform to coordinate incident response and case collection Cortex - analysis of events based indicators of compromise MISP - shared threat intelligence platform Shuffle - automation platform ThePhish - email analysis and triage Wazuh - endpoint monitoring Assembly_Line - file analysis Cuckoo_Sandbox - automated sandbox analysis environments The initial install takes care of many of the tools and will provide a backend database for storing case information. The details and highlights of configurations can be found at the following link. The template used includes an automation platform as well as integrations with MISP for sharing information with other SOCs. Docker-Templates/docker/thehive4-cortex3-misp-shuffle at main · TheHive-Project/Docker-Templates · GitHub\nAfter cloning the above repo with\ngit clone https://github.com/TheHive-Project/Docker-Templates/tree/main/docker/thehive4-cortex3-misp-shuffle\nyou can also clone ThePhish:\ngit clone https://github.com/emalderson/ThePhish.git\nIf there are any later configuration issues, you can find the repository here: ThePhish/docker at master · emalderson/ThePhish · GitHub\nThe docker-compose.yaml file will then need to be customized to include the following information in order to add ThePhish to the template.\nthephish: image: emalderson/thephish:latest container_name: thephish restart: unless-stopped depends_on: - thehive - cortex - misp ports: - \u0026lsquo;0.0.0.0:8081:8080\u0026rsquo; volumes: - ./ThePhish/app/analyzers_level_conf.json:/root/thephish/analyzers_level_conf.json - ./ThePhish/app/configuration.json:/root/thephish/configuration.json - ./ThePhish/app/whitelist.json:/root/thephish/whitelist.json\nI added this as the last container in the sequence and there were no issues pulling the image. As there are a multitude of containers that will be hosting web applications, the port mappings will need to be adjusted to ensure there is no unecessary overlap, which is where Portainer helps out by making it simpler to identify these kinds of issues and jump between container logs.\nThe default port mappings can be found in the initial docker-compose.yaml file, though I needed to change the mappings for MISP as port 8080 and 443 were occupied (there were also issues with Shuffle seemingly accessing port 443, which caused MISP to not be contactable); the Shuffle frontend as common ports were used; and ThePhish for the same reason.\nAfter all ports have been mapped, you can choose to select 0.0.0.0 or your static ip as the route. I found that adding the specific ip sometimes help get around issues where the container frontend was unreachable.\nAfter customizing the configurations, you can build the apps by changing to the parent directory (where the docker-compose.yaml is located if you are not already there) and running docker compose up -d (may need to \u0026lsquo;sudo\u0026rsquo; depending on how you configured docker) to begin the pull and build of all of the mentioned containers. You can check the status of the containers in the Portainer UI (reachable at https://server_ip:port_number). If there are issues with the containers, you can focus on them individually or bring them all down with docker compose down. You can then change the configuration in the docker-compose.yaml to remove any conflicts. When I integrated ThePhish, I did have some issues with conflicting files/directories, but this was likely due to trying to run the container for ThePhish in a separate compose and the paths to the configurations being misaligned. It might therefore be necessary to check the volume paths to ensure they are pointing in the right direction.\nIn its current iteration, ThePhish is designed to rely on a gmail account as the staging area for malicious emails to be sent. In order to change this, the ability to drag/drop or select local files could be added to the flask code that forms the basis of the app. However, for this setup a gmail address was created and used to contact the server. The configuration steps can be found here: ThePhish/docker at master · emalderson/ThePhish · GitHub After adding the gmail account details and app password found in account management to the configuration.json file in the app directory within the container, you can also add the URL and API of TheHive and the Cortex instance. The inbox of the account will then be able to pass .eml files to the analyzers and integrate with TheHive and Cortex by automatically accessing analyzers and using a case template to send a response email once the mailer responder is accurately configured in Cortex. Cortex will need the email address to use, the host, use of port 587, and the SMTP user and password configured.\nUser creation in TheHive, Cortex, and MISP #\rDefault user logins for each app can be found in the docker template readme on github.\nTo create a user in TheHive, you will first need to create an organization to assign them to. After using the default account to sign in, you can create the first organization and users by clicking through the UI with options generally in the top banner. You can set their passwords and generate API tokens that can be placed into app configuration files if needed.\nSimilarly, for Cortex, you will need to create an organization and users in the same manner. The API token for the app instance can then be placed into the application.conf file within TheHive\u0026rsquo;s directory on the server (for me this was located within my home directory where I cloned the template repo). This is also where you will need to add details about the MISP instance, such as the API key and the IP address/port where the instance can be reached.\nTo integrate MISP, grab the API key after logging in with the default credentials and creating the first organization. After adding the info to the application.conf file the integrations between the three apps should be ready to go. You will be able to spend some time looking at analyzers to set up and responders you would like to use. The configuration of some analyzers is done automatically.\nAdding the Shuffle webhook #\rAs there are a lot of apps included in this version of Shuffle, the container may have issues with memory and caching of the apps on the initial load. Pausing the container and giving it some time to process the cache seemed to work and allowed the container to become more stable. As outlined in the docker template documentation, the webhook uri will need to be put into the application.conf file specifying the organization in TheHive. Webhooks are also disabled by default and will need to be enabled following the instructions in the readme. Unfortunately, the link they use for how do this specifically is broken, but you can use the example listed as a framework for how to fill in your user details and find the listed file.\nread -p \u0026lsquo;Enter the URL of TheHive: \u0026rsquo; thehive_url read -p \u0026lsquo;Enter your login: \u0026rsquo; thehive_user read -s -p \u0026lsquo;Enter your password: \u0026rsquo; thehive_password\n`curl -XPUT -u$thehive_user:$thehive_password -H \u0026lsquo;Content-type: application/json\u0026rsquo; $thehive_url/api/config/organisation/notification -d ' { \u0026ldquo;value\u0026rdquo;: [ { \u0026quot;delegate\u0026quot;: false, \u0026ldquo;trigger\u0026rdquo;: { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;AnyEvent\u0026rdquo;}, \u0026quot;notifier\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;webhook\u0026quot;, \u0026quot;endpoint\u0026quot;: \u0026quot;local\u0026quot; } } ] }``\nOnce the webhooks are enabled you should be able to begin making workflows in the Shuffle frontend.\nWazuh in docker #\rThe next piece of the SOC is Wazuh for endpoint management. The steps can be found here: Wazuh Docker deployment - Deployment on Docker · Wazuh documentation The path I followed was single-node deployment. Once the repo has been cloned, change to the single node directory and run the command to generate certificates using a provided container docker-compose -f generate-indexer-certs.yml run --rm generator. You can then go to the directory with the docker-compose.yaml and check the configuration to see if there are any port assignment clashes. There might also be a clash within the opensearch_dashboards.yaml file in the /single-node/config/wazuh_dashboard directory. You can also change the information for the user login and password within the yaml file before running the group of containers with docker compose up -d. Once set up, you can begin to add agents to the network and begin collecting data related to your endpoints. The default configuration contains a lot of rules that collect data related to privilege escalation and you can add some responses to common attacks such as a brute force or sql injection attack. The granularity you can achieve with these rules seems quite high. Results can be filtered by rule_id among a host of other options to identify relevant data and to view the effectiveness of a given rule. Rules can be added in /var/ossec/etc/rules/. There is a large repository on github that has collections of rules that could be used as the basis for a production configuration. The current up to date list of rules can be found here: wazuh/ruleset at master · wazuh/wazuh · GitHub\nAssembly Line #\rThe tools to run Assembly Line in docker can be found here: Docker - Assemblyline 4 (cybercentrecanada.github.io). The setup is similar to the other containerized tools and requires a python environment, docker compose, and a firewall configuration for the minimal install. After these changes, clone the repo with git clone: git clone https://github.com/CybercentreCanada/assemblyline-docker-compose.gitand change into the deployments/assemblyline directory to begin making changes to the config.yaml file. It is set to run as a single node by default and has default login credentials that can be changed within the .env file inside of the deployment directory.\nThe next step is to create your https certificates or assign them from your local CA. If creating your own, this can be done with openssl req -nodes -x509 -newkey rsa:4096 -keyout ~/deployments/assemblyline/config/nginx.key -out ~/deployments/assemblyline/config/nginx.crt -days 365 -subj \u0026quot;/C=CA/ST=Ontario/L=Ottawa/O=CCCS/CN=assemblyline.local\u0026quot;.\nWhen inside of the deployments/assemblyline directory, you can compose the build and the web interface will come up, reachable at the chosen port. After logging in, you can then configure which services you want to use from the ones available. The defaults have some utility and can break down .exe files and weblinks among other file types. Many of the tools do not require added configuraton, but the most useful will require some added API information and port connections (e.g., Yara). In order to get the most out of Cuckoo, for example, there is a decent amount of configuration required and the server you are using will have fairly high resource requirements.\nCuckoo Sandbox #\rIn order to create an environment that could be used to track the effects of a file/attachment in operation, you can setup up a Cuckoo VM which will allow you to then pass information to Assembly Line. This also allows you to monitor attempted network activity to some extent and to test the file within different OS environments. On the linux server I was using, this starts with creating a VM using KVM and assigning it an ip within an isolated network.\nThe configuration for the VM can be automated with a tool called VMCloak (documentation here: Welcome to VMCloak’s documentation! — VMCloak 4.x documentation) to allow you to quickly spin up new environments for testing. I did not use this approach as I was relying on a single server and the configuration likely would have required more resources than I had available. Using VMCloak would be a better production solution as it would streamline a lot of the configurations needed, and once 2/3 machine configurations that adequately duplicate the operating environments you want to focus on are made, they can be used within both Cuckoo and Assembly Line more easily.\n","date":"9 December 2023","permalink":"/portfolio-pages-public/posts/thehive/","section":"Posts","summary":"During a work term, I was given the chance to build out some open source tools that can be used to create a streamlined security operations center.","title":"TheHive"},{"content":"","date":"1 January 0001","permalink":"/portfolio-pages-public/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"1 January 0001","permalink":"/portfolio-pages-public/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"1 January 0001","permalink":"/portfolio-pages-public/series/","section":"Series","summary":"","title":"Series"},{"content":"","date":"1 January 0001","permalink":"/portfolio-pages-public/tags/","section":"Tags","summary":"","title":"Tags"}]