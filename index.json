[{"content":"Bio #\rCurrent cybersecurity student with a background in teaching looking to explore and learn more about threat hunting, red teaming, and incident response.\nFirst fell in love with technology as a youngster by being enthralled by the speed of a blue hedgehog darting about a screen (an excellent distraction device employed by family). Since then, have taken a roundabout journey into tech and currently working towards getting certified in the area of penetration testing.\nThe aspect of giving back to the community in this field really appeals to me, and I look forward to sharing discoveries on my way with those already in the field and those like me who are finding their path into it.\n","date":"9 December 2023","permalink":"/portfolio-pages-public/posts/about-me/","section":"Posts","summary":"Bio #\rCurrent cybersecurity student with a background in teaching looking to explore and learn more about threat hunting, red teaming, and incident response.","title":"About Me"},{"content":"","date":"9 December 2023","permalink":"/portfolio-pages-public/","section":"b1p01arc0nf1d3nc3 - Portfolio","summary":"","title":"b1p01arc0nf1d3nc3 - Portfolio"},{"content":"","date":"9 December 2023","permalink":"/portfolio-pages-public/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"The Red Team Pathway on TryHackMe is a detailed and thorough exploration of a multitude of concepts required for red teaming. The pathway involves a wide range of rooms that offer excellent descriptions of reconnaissance, initial access techniques, post compromise escalation and pivoting, host evasion, network security evasion, and Active Directory compromise. The breadth of detail in these rooms is fantastic for those looking to take a closer look at red teaming concepts. Additionally, there are lots of practical examples and exercises in each room, culminating in Part 6 of the pathway, a very enjoyable collection of miniature network environments demonstrating how to breach, enumerate, pivot, exploit, and establish persistence when targeting Active Directory infrastructure.\nTo provide a full walkthrough of the experience of completing these challenge rooms could fill volumes, so below are summaries of the rooms that either most appealed to me or those that stood out in terms of the learning experience provided.\nPart 1 - Red Team Fundamentals #\rIntroductory section aimed to set the foundation for the various stages required for a red team engagement.\rIn this initial section, many of the concepts will be familiar to those studying information security, though the emphasis on operational security does well to set up the scope for all of the rooms that follow, making it easy to understand why certain more stealthy methods of attack are especially useful when performing a red team exercise. The most interesting room for me in this part of the pathway was the \u0026lsquo;Intro to C2\u0026rsquo; room. This room describes the use of multihandlers and port forwarding to manage traffic between the C2 server/listener and the target machine, and the \u0026lsquo;Common C2 Frameworks\u0026rsquo; subsection provides a good overview of options. Among the frameworks described, Sliver stood out as one of the tools worth investing further time in. In fact, this is a tool that I would later include in a malware development project as the C2 backbone.\nPart 2 - Initial Access #\rThis section is full of ways to approach gaining initial access to an environment.\rThis section of the pathway provides some great practical examples of payload development and delivery techniques, including some exploration of the use of PowerShell (though this really just scratches the surface). Beyond this, the \u0026lsquo;Phishing\u0026rsquo; room leverages GoPhish to provide a great practical example of generating a phishing campaign for a red team engagement. Similar to the content related to PowerShell, this still mostly functions as an introduction to the process of generating a believable campaign, but the practical element definitely adds to the learning experience (a common plus throughout the pathway).\nPart 3 - Post Compromise #\rAssess a machine you land on and gain privileges to then set up a stronger foothold.\rThis part of the pathway was perhaps one of the most valuable areas to explore and rounded out prior knowledge I had in relation to working your way up through a machine. I especially enjoyed the escalation and persistence subsections, each of which detailed some common quick wins and some of the most common misconfigurations that lead to escalation and persistence opportunities. The techniques explore well-know paths that can help round out your toolbox, including backdooring the logon screen and taking advantage of MSSQL servers to set up. These sections exemplify the strengths of this pathway, as the exposure to a broad range of applicable techniques and the opportunity to employ them helps develop a sense of confidence picking up new exploits and then figuring out how to use them. I feel like I definitely became more adept at identifying the different weaknesses of certain systems and then linking that to how I would attempt to either escalate privileges or set up a form of persistence.\nThis section also includes the first self-contained network environment to attack. The \u0026lsquo;Lateral Movement and Pivoting\u0026rsquo; room containing this network also includes a more detailed look at tunneling and port forwarding. Completing this network helped solidify my understanding of proxying and forwarding commands from an attack box to machines that weren\u0026rsquo;t immediately available. Also, exposure to a range of tools in this area provides options when trying to pivot between different devices. The best tool covered in this area is probably proxychains, which is incredibly useful when the tools you want to run can\u0026rsquo;t be found on the machine you are attacking.\nOne last area from this subsection that was of particular interest was data exfiltration, a regular bugbear of many an enterprise. Among the more common methods, such as TCP sockets and SSH, the inclusion of ICMP and DNS is a nice thought-provoking addition that results in you questioning whether any protocol could eventually be used to stealthily and patiently move data out of a network.\nPart 4 - Host Evasions #\rThis long list is full of great content that fleshes out a variety of ways to perform less obvious execution and generate stealthier payloads.\rPart 4 is probably the section that will take the longest to complete, though the time investment pays off in terms of how much it offers. This section doesn\u0026rsquo;t just go over simple obfuscation but also goes into a good level of detail about how to code specific elements of an exploit so that various forms of detection are rendered useless. Of all the sections within the pathway, this one challenged me the most in terms of being able to find solutions to the more open challenges included. I plan to revisit this section in the future to not just hopefully recognize how far I have come, but also to find further novel ways of approaching the problems.\nIn addition to coding examples for payload development, this section also includes descriptions of common LOLBINs that are used to perform malicious actions in ways that are more difficult to detect. Using a built-in tool to decode your payload feels very rewarding when you can successfully chain together techniques and rely on the utilities already on the target machine.\nIf I had to recommend rooms from this section, I would probably recommend the \u0026lsquo;Runtime Detection Evasion\u0026rsquo; and \u0026lsquo;Obfuscation Principles\u0026rsquo; rooms. These challenge rooms provide lots of practical experience in modifying your code to bypass specific forms of detection. They also represent a significant jump in difficulty level which is especially rewarding to overcome, helping you feel ready for the network rooms in the final part of the pathway.\nPart 5 - Network Security Evasion #\rShort but sweet, the \u0026lsquo;Network Security Evasion\u0026rsquo; section builds well on the earlier more endpoint-focused sections.\rThis is the shortest section of the pathway but still provides a useful look into evading sandbox environments, something especially important once you have invested significant time into a payload and don\u0026rsquo;t want it triggering until it can actually affect the target. The \u0026lsquo;Sandbox Evasion\u0026rsquo; room also ties in well with the foundations developed in the \u0026lsquo;Obfuscation Principles\u0026rsquo; room. This occurs often throughout the pathway, with one section introducing a concept, another applying it in practice, and another building on it in combination with some of the other concepts learned.\nPart 6 - Compromising Active Directory #\rThe AD networks included in the last section are the perfect place to put theory into action.\rThis section presents the best opportunity to put all of the different elements together. The rooms model a full kill chain, moving from enumeration and initial access to lateral movement and persistence. This is a great opportunity to explore the concept of continually establishing various forms of persistence and then using the foothold to gain the next step toward the final objective. This collection of rooms also functions as a great chance to determine whether or not there are specific areas that require more focus.\nLooking back at the pathway as a whole, this is a fitting end in that it places all of the different concepts covered into the main type of environment that will be attacked in red team engagements. This section also builds on those concepts and introduces further techniques that apply specifically to Active Directory. These networks are contained enough that you always know what the current and final goal are, allowing you to connect some ideas that may not have seemed as related beforehand.\nIn terms of takeaways, this pathway was fantastic as a learning tool to become more confident in the methodology that would be required to perform a red team engagement. I was impressed by the breadth of content offered and the areas included. This comprehensive coverage and the detailed nature of the explanations allows you to apply the new techniques quickly in a realistic setting and determine where those kinds of techniques could be used in future engagements. The wide range of tools mentioned was also a major benefit, helping round out areas where previously I would not have been aware of suitable programs or utilities. Overall, the pathway was superb and is an experience I would wholeheartedly recommend to others looking to expand their skills and knowledge in the area of red teaming.\n","date":"9 December 2023","permalink":"/portfolio-pages-public/posts/thm-red-team-pathway/","section":"Posts","summary":"The Red Team Pathway on TryHackMe is a detailed and thorough exploration of a multitude of concepts required for red teaming.","title":"THM Red Team Pathway"},{"content":"Systems vs. Security #\rDuring a cross-program project between systems administration and cyber security students, the cyber security students were tasked with breaking into a stand-alone server administered by the systems students. We broke off into teams and were given the IP address of our target server.\nOur team chose the name \u0026lsquo;Ping Pirates\u0026rsquo; and began enumerating the target machine. Using common tools such as nmap to find out which ports were open and what services were running, we discovered a limited range of ports and a dashboard running on a random ephemeral port.\nAmong the open ports was port 21, potentially offering ftp access. We enumerated this port more closely and found that it offered anonymous login, allowing us to access a staff list which could then be used to generate a list of usernames. Once we generated a list of potential usernames following common patterns (first.last, last.first, firstinitial.last, etc.), we did some quick brute force attacks to see if there were any accounts with weak passwords. After running these brute attacks with larger wordlists, it became apparent that this would not be a viable means of gaining initial access; this was made painfully obvious when we later discovered that all passwords were above 14 characters in length, with the most privileged passwords randomly generated and up to 20 characters.\nWe then changed focus and tried to assess how the systems students would be using the machine and looked into intercepting traffic to gain more intel. We knew they had the standard RDP port open and that they had a dashboard for a monitoring platform. We decided the best thing to do would be to try and relay traffic to either (a) intercept a login to their monitoring platform, as it did not use https, or (b) use a tool like Responder to relay traffic and potentially gain hashes.\n","date":"9 December 2023","permalink":"/portfolio-pages-public/posts/red-vs.-blue/","section":"Posts","summary":"Systems vs.","title":"Red vs. Blue"},{"content":"This will be a walkthrough of the malware dev project.\n","date":"9 December 2023","permalink":"/portfolio-pages-public/posts/dll-sideloading/","section":"Posts","summary":"This will be a walkthrough of the malware dev project.","title":"Dll Sideloading"},{"content":"How to play and enjoy Backdoors and Breaches #\rBackdoors and Breaches is a fantastic incident response activity that challenges participants to solve a multi-faceted cyber incident by making decisions and watching everything go wrong (well, at least some of the time).\nThe activity if free to try and is made by the fabulous folks over at Black Hills Information Security, who can be found here. Amazingly, the activity is even hosted online for all to use, and the web version makes it easy to host a remote session without requiring extensive preparation (you can find out more about the different ways to play here).\nIn terms of how the activity works, the incident responders (the participants) have 10 rounds to try and deal with the incident before it becomes too impactful. They need to discuss what they would like to and prioritize remediatory actions to attempt a full resolution. They can use procedure cards, such as SIEM log analysis and network threat hunting, to try and uncover one of the four elements of the incident. If the team successfully manage to uncover all four elements of the incident in time then they have handled the incident. There are a few other rules which can significantly affect play, and the main rules are summarized below.\nRules #\rWork as a team to discover all 4 cards to respond to the incident. 10 rounds to discover all 4 event cards. Actions can be chosen from the procedure cards. Actions must be reached via consensus (can also ask for specific clarification from the incident leader). A dice roll is made to determine if an action is successful or not. 11 and over is a success, while 10 and below is a failure. There is a 3-round cooldown after using a card to prevent the same card being used repeatedly. An \u0026lsquo;inject\u0026rsquo; card is added on a roll of 1 or 20, or after 3 consecutive failures. Inject cards add a random element to the game and can be positive or very destructive. These cards often lead to the most significant learning as participants are forced to react quickly to the change. Though not necessarily a rule, the following are recommendations to get more out of the experience:\nAfter a failure, ask for why that might have happened. Participants will often be able to give a justification for you, especially if they know of a weakness in their environment. The failure could, for example, be due to resource constraints, human errors, or policy changes. After the game, go through and assess if what happened was realistic. Participants will generally respond with a Yes, No, or Maybe. Always clarify Maybe. The time spent clarifying is well worth it due to the extra insight participants will get into their environment and you will also learn how to make the activity more representative next time. Allow for unique elements and changes to rules. The rules are a solid framework, but as the incident leader you have the ability to incorporate the suggestions and ideas of the participants to make the activity more successful. This could include revealing a slight hint due to the specificity of what the participants attempt to do, adding the ability to call a consultant if it fits the overall narrative, or guaranteeing a success for a procedure that the participants show excellence in. The way the activity is run by the incident leader obviously has a large impact on the success of the activity, but it is also important to get participants to buy in to the activity in the early stages by adding humor or appealing to what they are looking for. As a member of a cybersecurity club, this has often revolved around incorporation of our mascot: a hacking goose. This goose has been added to a range of different companies, and I always try my best to try and make a logo that includes the mascot and a slightly altered company name. This hopefully not only prompts a smile, but also sets the context for where the incident and the activity take place.\nAdding flavor with customized companies helps players become more involved in the scenario while also providing scope for the activity.\rSometimes, the sillier the company sounds, the more invested participants become.\rFrom the chances I\u0026rsquo;ve had to lead this activity, I can definitely vouch for its applicability to various groups in the cyber space. It has been a great tool for students as it allows them to apply the knowledge they are gaining throughout whatever course they may be doing to an activity where they are forced to justify decisions. Talking through options help reinforce learning and builds a degree of confidence when sharing this kind of information with those less focused on cyber. I can also see the value it would hold for incident response teams at different organizations, who would benefit from the opportunity to review incident response plans and identify areas that might be a bit foggy, thus prompting them to clarify procedures and communication trees to enable them to effectively respond to incidents.\nIf you can, I wholeheartedly recommend trying the activity out. If you want some tips, check out the video provided by BHIS to find out more.\n","date":"9 December 2023","permalink":"/portfolio-pages-public/posts/backdoors-and-breaches/","section":"Posts","summary":"How to play and enjoy Backdoors and Breaches #\rBackdoors and Breaches is a fantastic incident response activity that challenges participants to solve a multi-faceted cyber incident by making decisions and watching everything go wrong (well, at least some of the time).","title":"Backdoors and Breaches"},{"content":"During a work term, I was given the chance to build out some open source tools that can be used to create a streamlined security operations center. It was a very valuable experience and helped me develop a better understanding of tooling available to organizations that may have resource limitations.\nHere are some of the configuration steps and code that was used in the setup of the Hive server, a self-contained testing server to explore open-source SOC options.\nBase OS: CentOS 7. Requirements: Depending on the number of products you want to test at once, this could range from 8 - 32 GB.\nDocker engine: simple install using yum: sudo yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\nOnce docker is set up, start the service and check the inital hello world container can run. If it does, then we can start building the template for the SOC from the hosted page on github.\nPortainer #\rUsing Portainer will make managing the large number of containers and images simpler. The container for Portainer can be installed by following the steps in this link:\nInstall Portainer with Docker on Linux - Portainer Documentation\nThe basic steps are given below.\ndocker volume name_of_data_storage_for_portainer\nThe above command will create a docker volume for Portainer. We can then run the docker command to pull in the image and map the ports we will use to connect to the web interface for managing containers.\nI used:\ndocker run -d -p 8000:8000 -p 9443:9443 --name portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v name_of_data_storage_for_portainer:/data portainer/portainer-ce:2.9.3\nthough your command may look different depending on the versions and the ports you select.\nFor the web interface, Portainer will automatically create a self-signed certificate, but this can be changed both during and after installation, should this be a production environment.\nSOC Components #\rThe following tools were used to create the SOC, providing a wide range of functionality for small teams of security analysts.\nTheHive - incident platform to coordinate incident response and case collection Cortex - analysis of events based indicators of compromise MISP - shared threat intelligence platform Shuffle - automation platform ThePhish - email analysis and triage Wazuh - endpoint monitoring Assembly_Line - file analysis Cuckoo_Sandbox - automated sandbox analysis environments The initial install takes care of many of the tools and will provide a backend database for storing case information. The details and highlights of configurations can be found at the following link. The template used includes an automation platform as well as integrations with MISP for sharing information with other SOCs. Docker-Templates/docker/thehive4-cortex3-misp-shuffle at main · TheHive-Project/Docker-Templates · GitHub\nAfter cloning the above repo with\ngit clone https://github.com/TheHive-Project/Docker-Templates/tree/main/docker/thehive4-cortex3-misp-shuffle\nyou can also clone ThePhish:\ngit clone https://github.com/emalderson/ThePhish.git\nIf there are any later configuration issues, you can find the repository here: ThePhish/docker at master · emalderson/ThePhish · GitHub\nThe docker-compose.yaml file will then need to be customized to include the following information in order to add ThePhish to the template.\nthephish: image: emalderson/thephish:latest container_name: thephish restart: unless-stopped depends_on: - thehive - cortex - misp ports: - \u0026lsquo;0.0.0.0:8081:8080\u0026rsquo; volumes: - ./ThePhish/app/analyzers_level_conf.json:/root/thephish/analyzers_level_conf.json - ./ThePhish/app/configuration.json:/root/thephish/configuration.json - ./ThePhish/app/whitelist.json:/root/thephish/whitelist.json\nI added this as the last container in the sequence and there were no issues pulling the image. As there are a multitude of containers that will be hosting web applications, the port mappings will need to be adjusted to ensure there is no unecessary overlap, which is where Portainer helps out by making it simpler to identify these kinds of issues and jump between container logs.\nThe default port mappings can be found in the initial docker-compose.yaml file, though I needed to change the mappings for MISP as port 8080 and 443 were occupied (there were also issues with Shuffle seemingly accessing port 443, which caused MISP to not be contactable); the Shuffle frontend as common ports were used; and ThePhish for the same reason.\nAfter all ports have been mapped, you can choose to select 0.0.0.0 or your static ip as the route. I found that adding the specific ip sometimes help get around issues where the container frontend was unreachable.\nAfter customizing the configurations, you can build the apps by changing to the parent directory (where the docker-compose.yaml is located if you are not already there) and running docker compose up -d (may need to \u0026lsquo;sudo\u0026rsquo; depending on how you configured docker) to begin the pull and build of all of the mentioned containers. You can check the status of the containers in the Portainer UI (reachable at https://server_ip:port_number). If there are issues with the containers, you can focus on them individually or bring them all down with docker compose down. You can then change the configuration in the docker-compose.yaml to remove any conflicts. When I integrated ThePhish, I did have some issues with conflicting files/directories, but this was likely due to trying to run the container for ThePhish in a separate compose and the paths to the configurations being misaligned. It might therefore be necessary to check the volume paths to ensure they are pointing in the right direction.\nIn its current iteration, ThePhish is designed to rely on a gmail account as the staging area for malicious emails to be sent. In order to change this, the ability to drag/drop or select local files could be added to the flask code that forms the basis of the app. However, for this setup a gmail address was created and used to contact the server. The configuration steps can be found here: ThePhish/docker at master · emalderson/ThePhish · GitHub After adding the gmail account details and app password found in account management to the configuration.json file in the app directory within the container, you can also add the URL and API of TheHive and the Cortex instance. The inbox of the account will then be able to pass .eml files to the analyzers and integrate with TheHive and Cortex by automatically accessing analyzers and using a case template to send a response email once the mailer responder is accurately configured in Cortex. Cortex will need the email address to use, the host, use of port 587, and the SMTP user and password configured.\nUser creation in TheHive, Cortex, and MISP #\rDefault user logins for each app can be found in the docker template readme on github.\nTo create a user in TheHive, you will first need to create an organization to assign them to. After using the default account to sign in, you can create the first organization and users by clicking through the UI with options generally in the top banner. You can set their passwords and generate API tokens that can be placed into app configuration files if needed.\nSimilarly, for Cortex, you will need to create an organization and users in the same manner. The API token for the app instance can then be placed into the application.conf file within TheHive\u0026rsquo;s directory on the server (for me this was located within my home directory where I cloned the template repo). This is also where you will need to add details about the MISP instance, such as the API key and the IP address/port where the instance can be reached.\nTo integrate MISP, grab the API key after logging in with the default credentials and creating the first organization. After adding the info to the application.conf file the integrations between the three apps should be ready to go. You will be able to spend some time looking at analyzers to set up and responders you would like to use. The configuration of some analyzers is done automatically.\nAdding the Shuffle webhook #\rAs there are a lot of apps included in this version of Shuffle, the container may have issues with memory and caching of the apps on the initial load. Pausing the container and giving it some time to process the cache seemed to work and allowed the container to become more stable. As outlined in the docker template documentation, the webhook uri will need to be put into the application.conf file specifying the organization in TheHive. Webhooks are also disabled by default and will need to be enabled following the instructions in the readme. Unfortunately, the link they use for how do this specifically is broken, but you can use the example listed as a framework for how to fill in your user details and find the listed file.\nread -p \u0026lsquo;Enter the URL of TheHive: \u0026rsquo; thehive_url read -p \u0026lsquo;Enter your login: \u0026rsquo; thehive_user read -s -p \u0026lsquo;Enter your password: \u0026rsquo; thehive_password\n`curl -XPUT -u$thehive_user:$thehive_password -H \u0026lsquo;Content-type: application/json\u0026rsquo; $thehive_url/api/config/organisation/notification -d ' { \u0026ldquo;value\u0026rdquo;: [ { \u0026quot;delegate\u0026quot;: false, \u0026ldquo;trigger\u0026rdquo;: { \u0026ldquo;name\u0026rdquo;: \u0026ldquo;AnyEvent\u0026rdquo;}, \u0026quot;notifier\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;webhook\u0026quot;, \u0026quot;endpoint\u0026quot;: \u0026quot;local\u0026quot; } } ] }``\nOnce the webhooks are enabled you should be able to begin making workflows in the Shuffle frontend.\nWazuh in docker #\rThe next piece of the SOC is Wazuh for endpoint management. The steps can be found here: Wazuh Docker deployment - Deployment on Docker · Wazuh documentation The path I followed was single-node deployment. Once the repo has been cloned, change to the single node directory and run the command to generate certificates using a provided container docker-compose -f generate-indexer-certs.yml run --rm generator. You can then go to the directory with the docker-compose.yaml and check the configuration to see if there are any port assignment clashes. There might also be a clash within the opensearch_dashboards.yaml file in the /single-node/config/wazuh_dashboard directory. You can also change the information for the user login and password within the yaml file before running the group of containers with docker compose up -d. Once set up, you can begin to add agents to the network and begin collecting data related to your endpoints. The default configuration contains a lot of rules that collect data related to privilege escalation and you can add some responses to common attacks such as a brute force or sql injection attack. The granularity you can achieve with these rules seems quite high. Results can be filtered by rule_id among a host of other options to identify relevant data and to view the effectiveness of a given rule. Rules can be added in /var/ossec/etc/rules/. There is a large repository on github that has collections of rules that could be used as the basis for a production configuration. The current up to date list of rules can be found here: wazuh/ruleset at master · wazuh/wazuh · GitHub\nAssembly Line #\rThe tools to run Assembly Line in docker can be found here: Docker - Assemblyline 4 (cybercentrecanada.github.io). The setup is similar to the other containerized tools and requires a python environment, docker compose, and a firewall configuration for the minimal install. After these changes, clone the repo with git clone: git clone https://github.com/CybercentreCanada/assemblyline-docker-compose.gitand change into the deployments/assemblyline directory to begin making changes to the config.yaml file. It is set to run as a single node by default and has default login credentials that can be changed within the .env file inside of the deployment directory.\nThe next step is to create your https certificates or assign them from your local CA. If creating your own, this can be done with openssl req -nodes -x509 -newkey rsa:4096 -keyout ~/deployments/assemblyline/config/nginx.key -out ~/deployments/assemblyline/config/nginx.crt -days 365 -subj \u0026quot;/C=CA/ST=Ontario/L=Ottawa/O=CCCS/CN=assemblyline.local\u0026quot;.\nWhen inside of the deployments/assemblyline directory, you can compose the build and the web interface will come up, reachable at the chosen port. After logging in, you can then configure which services you want to use from the ones available. The defaults have some utility and can break down .exe files and weblinks among other file types. Many of the tools do not require added configuraton, but the most useful will require some added API information and port connections (e.g., Yara). In order to get the most out of Cuckoo, for example, there is a decent amount of configuration required and the server you are using will have fairly high resource requirements.\nCuckoo Sandbox #\rIn order to create an environment that could be used to track the effects of a file/attachment in operation, you can setup up a Cuckoo VM which will allow you to then pass information to Assembly Line. This also allows you to monitor attempted network activity to some extent and to test the file within different OS environments. On the linux server I was using, this starts with creating a VM using KVM and assigning it an ip within an isolated network.\nThe configuration for the VM can be automated with a tool called VMCloak (documentation here: Welcome to VMCloak’s documentation! — VMCloak 4.x documentation) to allow you to quickly spin up new environments for testing. I did not use this approach as I was relying on a single server and the configuration likely would have required more resources than I had available. Using VMCloak would be a better production solution as it would streamline a lot of the configurations needed, and once 2/3 machine configurations that adequately duplicate the operating environments you want to focus on are made, they can be used within both Cuckoo and Assembly Line more easily.\n","date":"9 December 2023","permalink":"/portfolio-pages-public/posts/thehive/","section":"Posts","summary":"During a work term, I was given the chance to build out some open source tools that can be used to create a streamlined security operations center.","title":"TheHive"},{"content":"","date":"1 January 0001","permalink":"/portfolio-pages-public/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"1 January 0001","permalink":"/portfolio-pages-public/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"1 January 0001","permalink":"/portfolio-pages-public/series/","section":"Series","summary":"","title":"Series"},{"content":"","date":"1 January 0001","permalink":"/portfolio-pages-public/tags/","section":"Tags","summary":"","title":"Tags"}]